{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the NeuArm data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not stored in the present GitHub repository. \n",
    "\n",
    "The data is stored in a local directory on my machine.\n",
    "\n",
    "I use a simlink to the data directory in my local machine. \n",
    "\n",
    "The simlink was created with: \n",
    "\n",
    "1. Move to the directory where you want to create the simlink, here in the `data` folder.  \n",
    "\n",
    "1. Create the simlink with: `ln -s <path to data directory> <current directory>`  \n",
    "Here: `ln -s ~/ExperimentalData/NeuArm ./`  \n",
    "This creates a simlink named `NeuArm` in the current directory, which is the `data` folder.\n",
    "\n",
    "1. Rename the simlink to `NeuArm.lnk` and add `*.lnk` it to the `.gitignore` file.  \n",
    "This is done to avoid that the data is added to the GitHub repository. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore  the files in all the folders that are in the data folder\n",
    "# print the name of the files found in each folder\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "verbose = False\n",
    "# utility function to print only if verbose is True\n",
    "def print_v(*args):\n",
    "    if verbose:\n",
    "        print(*args)\n",
    "\n",
    "# get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print_v(cwd)\n",
    "\n",
    "# if we are in the notebooks folder, go back to the root folder\n",
    "if os.path.basename(cwd) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "    cwd = os.getcwd()\n",
    "    print_v(cwd)\n",
    "\n",
    "# get the path to the data folder\n",
    "data_folder = os.path.join(cwd, 'data')\n",
    "print_v(data_folder)\n",
    "\n",
    "# recursively get the list of all the folders in the data folder\n",
    "folders = glob.glob(data_folder + '/**/', recursive=True)\n",
    "print_v(folders)\n",
    "\n",
    "# loop among the folders to fill in a list of *.xdf files\n",
    "dataFiles_NeuArm = []\n",
    "for folder in folders:\n",
    "    files = os.listdir(folder)\n",
    "    # print the name of the folder and the list of files as a column, file by file\n",
    "    print_v(folder)\n",
    "    for file in files:\n",
    "        print_v(file)\n",
    "        if file.endswith('.xdf'):\n",
    "            fullPath = os.path.join(folder, file)\n",
    "            relPath = os.path.relpath(fullPath, cwd)\n",
    "            dataFiles_NeuArm.append(relPath)\n",
    "\n",
    "# sort the list of files\n",
    "dataFiles_NeuArm.sort()\n",
    "for i in range(len(dataFiles_NeuArm)):\n",
    "    # print the index with 2 leading zeros, the name of the file with an f string\n",
    "    print(f'{i:02d} {dataFiles_NeuArm[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that takes a filename and returns the participantID and all information a dictionary\n",
    "def getConditionFromFileName(relFileName):\n",
    "    # get the path (if exists) and filename\n",
    "    path = os.path.dirname(relFileName)\n",
    "    fname = os.path.basename(relFileName)\n",
    "    fname, extension = os.path.splitext(fname)\n",
    "    # get the tokens separated by the underscore\n",
    "    tokens = fname.split('_')\n",
    "    # try to get the condition from the filename\n",
    "    try:\n",
    "        participantNumber = tokens[0]\n",
    "        participantName = tokens[1]\n",
    "        date = tokens[2]\n",
    "        session = tokens[3]\n",
    "        condition = tokens[4]\n",
    "        participantID = participantNumber + '_' + participantName\n",
    "        group = path.split(os.path.sep)[-1]\n",
    "        return {'participantID': participantID, 'date': date, 'session': session, 'condition': condition, 'group': group, 'fname': fname, 'extension': extension}\n",
    "    except Exception as e: \n",
    "        print(file, 'is incorrect (error:', e, ')')\n",
    "        return None\n",
    "\n",
    "# test the function\n",
    "fname = '001_LecGer_20210826_1_c.xdf'   # not path\n",
    "fname = dataFiles_NeuArm[0]             # with path (relative)\n",
    "tokens = getConditionFromFileName(fname)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that each *.xdf file in the `data` folder has a valid filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that each *.xdf file in the `data` folder has a valid filename\n",
    "for folder in folders:\n",
    "    files = os.listdir(folder)\n",
    "    for file in files:\n",
    "        if file.endswith('.xdf'):\n",
    "            tokens = getConditionFromFileName(file)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CSV file with all the information about the data files\n",
    "for each file, we want to parse the information that is in the file name and location, and store it in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def appendLineToConditionsFile(conditionsFile, fullFname):\n",
    "    relativeFname = os.path.relpath(fullFname, cwd)\n",
    "    tokens = getConditionFromFileName(relativeFname)\n",
    "    # protect the backslashes and commas before writing the line in CSV format\n",
    "    protectedFname = relativeFname.replace('\\\\', '\\\\\\\\')\n",
    "    protectedFname = protectedFname.replace(',', '\\\\,')\n",
    "    if tokens is not None:\n",
    "        txt = tokens['participantID'] + ',' + tokens['date'] \n",
    "        txt = txt + ',' + tokens['session'] + ',' + tokens['condition'] + ',' + tokens['group'] \n",
    "        txt = txt + ',' + tokens['fname'] + ',' + tokens['extension'] \n",
    "        txt = txt + ',' + relativeFname\n",
    "        txt = txt + '\\n'\n",
    "\n",
    "        f = open(conditionsFile, 'a')\n",
    "        f.write(txt)\n",
    "        f.close()\n",
    "\n",
    "# create the conditions file\n",
    "conditionsFile = 'data/NeuArm_conditions.csv'\n",
    "f = open(conditionsFile, 'w')\n",
    "f.write('participantID,date,session,condition,group,fname,extension,relativePath\\n')\n",
    "f.close()\n",
    "# loop among the folders\n",
    "for folder in folders:\n",
    "    files = os.listdir(folder)\n",
    "    for file in files:\n",
    "        fullpath = os.path.join(folder, file)\n",
    "        if file.endswith('.xdf'):\n",
    "            appendLineToConditionsFile(conditionsFile, fullpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CSV file in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# open the CSV file with the conditions in a numpy array\n",
    "conditionsFile = 'data/NeuArm_conditions.csv'\n",
    "conditions = np.genfromtxt(conditionsFile, delimiter=',', names=True, dtype=None, encoding=None)\n",
    "print(conditions.dtype.names)\n",
    "print(conditions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the data files contain the expected data streams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the expected data streams\n",
    "\n",
    "A data stream is uniquely identified by its *name* and *type*. Yet, the name of some data streams are not always the same: `NIC` changed to `LSLOutletStreamName` during the experiment...   \n",
    "Below, we use a list of list, where each sublist (line) contains the possible names and types of a data stream.\n",
    "\n",
    "    expected_data_stream = [\n",
    "        [ [name1, name2], [ type ] ], \n",
    "        ...\n",
    "        [ [name1, name2], [ type1, type2 ] ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of expected streams\n",
    "# each stream is a list of 2 elements: the first is a list of possible names, the second is the type\n",
    "# NB : this is because NIC changed to LSLOutletStreamName in the middle of the study \n",
    "expectedDataStreams = [\n",
    "    [['NIC-Accelerometer', 'LSLOutletStreamName-Accelerometer'] , 'Accelerometer'], \n",
    "    [['NIC-EEG', 'LSLOutletStreamName-EEG'], 'EEG'], \n",
    "    [['NIC-Markers', 'LSLOutletStreamName-Markers'], 'Markers'],\n",
    "    [['NIC-Quality', 'LSLOutletStreamName-Quality'], 'Quality'], \n",
    "\n",
    "    [['Oxysoft Event'], 'Event'], \n",
    "    [['Oxysoft'], 'NIRS'], \n",
    "\n",
    "    [['Mouse'], 'MoCap'], \n",
    "    [['Mouse'], 'Markers'], \n",
    "    [['MouseToNIC'],  'Markers'], \n",
    "\n",
    "    [['EuroMov-Markers-Kinect'], 'Markers'], \n",
    "    [['EuroMov-Mocap-Kinect'], 'MoCap'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data streams in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxdf\n",
    "\n",
    "def checkStreamsInXdfFile(data, expected, verbose=False):\n",
    "    # array of zeros of the same length as expected \n",
    "    iFound = [0 * i for i in range(len(expected))] \n",
    "    unexpectedStream = []\n",
    "    foundStream = []\n",
    "    for stream in data:\n",
    "        streamName = stream['info']['name'][0]\n",
    "        streamType = stream['info']['type'][0]\n",
    "        unexpectedStream.append([streamName, streamType])\n",
    "        for i in range(len(expected)):\n",
    "            expectedStreamName = expected[i][0]\n",
    "            expectedStreamType = expected[i][1]\n",
    "            if streamName in expectedStreamName and streamType in expectedStreamType:\n",
    "                iFound[i] = 1\n",
    "                foundStream.append([streamName, streamType])\n",
    "                unexpectedStream.pop()\n",
    "                break\n",
    "\n",
    "    def printFoundStreams():\n",
    "        print('found streams (order as in data):')\n",
    "        for i in range(len(foundStream)):\n",
    "            print('  ', foundStream[i][0], foundStream[i][-1])\n",
    "\n",
    "        print('found streams (order as in expected):')\n",
    "        for i in range(len(expected)):\n",
    "            if iFound[i] == 1:\n",
    "                print('  ', expected[i][0], expected[i][-1])\n",
    "\n",
    "    def printUnexpectedStreams():\n",
    "        print('unexpected streams (order as in data):')\n",
    "        for i in range(len(unexpectedStream)):\n",
    "            print('  ', unexpectedStream[i][0], unexpectedStream[i][-1])\n",
    "        if len(unexpectedStream) == 0:\n",
    "            print('  none')\n",
    "\n",
    "    def printMissingStreams():\n",
    "        print('missing streams (order as in expected):')\n",
    "        for i in range(len(iFound)):\n",
    "            if iFound[i] == 0:\n",
    "                print('  ', expected[i][0], expected[i][-1])\n",
    "        if all(iFound) == 1:\n",
    "            print('  none')\n",
    "\n",
    "    if verbose:\n",
    "        printFoundStreams()\n",
    "        printUnexpectedStreams()\n",
    "        printMissingStreams()\n",
    "    else:\n",
    "        if len(unexpectedStream) > 0:\n",
    "            printUnexpectedStreams()\n",
    "        if all(iFound) != 1:\n",
    "            printMissingStreams()\n",
    "\n",
    "# test the function\n",
    "\n",
    "fname = '017_BatJea_20211214_1_c' + '.xdf' # 11 streams\n",
    "fullpath = cwd + \"/data/NeuArm.lnk/Old_Healthy/017_BatJea_20211214_1_c.xdf\"\n",
    "\n",
    "fname = '018_ChaJea_20220202_1_c' + '.xdf' # 09 streams\n",
    "fullpath = cwd + \"/data/NeuArm.lnk/Old_Healthy/018_ChaJea_20220202_1_c.xdf\"\n",
    "\n",
    "# fname = \"010_LapBas_20201124_1_c\" + \".xdf\" \n",
    "# fullpath = cwd + \"//data/NeuArm.lnk/Young_Healthy/010_LapBas_20201124_1_c.xdf\"\n",
    "\n",
    "data, header = pyxdf.load_xdf(fullpath, synchronize_clocks=True, dejitter_timestamps=False, verbose=False)\n",
    "checkStreamsInXdfFile(data, expectedDataStreams, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that all the data files contain the expected data streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in conditions:\n",
    "    fullpath = os.path.join(cwd, condition['relativePath'])\n",
    "    data, header = pyxdf.load_xdf(fullpath, synchronize_clocks=True, dejitter_timestamps=False, verbose=False)\n",
    "    print(condition['relativePath'])\n",
    "    checkStreamsInXdfFile(data, expectedDataStreams)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
